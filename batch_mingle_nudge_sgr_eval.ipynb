{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0174bf00",
   "metadata": {},
   "source": [
    "### Batch synthesis with variation in speaker and on the feature axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8806f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "%matplotlib notebook\n",
    "import matplotlib.pylab as plt\n",
    "import IPython.display as ipd\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchsummary\n",
    "import csv\n",
    "import os\n",
    "from g2p_en import G2p\n",
    "import re\n",
    "import itertools as it\n",
    "import pandas as pd\n",
    "\n",
    "from tronduo.hparams import create_hparams\n",
    "hparams = create_hparams()\n",
    "hparams.global_mean = None\n",
    "hparams.distributed_run = False\n",
    "hparams.prosodic = True\n",
    "hparams.speakers= True\n",
    "hparams.feat_dim = 1\n",
    "hparams.feat_max_bg = 8\n",
    "hparams.n_speakers= 2\n",
    "hparams.speaker_embedding_dim = 8\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "import os\n",
    "import json\n",
    "from hifigan.env import AttrDict\n",
    "from hifigan.models import Generator\n",
    "MAX_WAV_VALUE = 32768.0\n",
    "device = 'cuda'\n",
    "from tronduo.hifigandenoiser import Denoiser\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d6caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tronduo.model import Tacotron2\n",
    "from tronduo.layers import TacotronSTFT, STFT\n",
    "from scipy.io import wavfile\n",
    "from tronduo.model_util import load_model\n",
    "from tronduo import text_to_sequence\n",
    "g2p = G2p()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d15598f",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e85eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow models\n",
    "#path = \"output/feat_ljsryan/\" # added f0 feature, normalised per speaker\n",
    "#path = \"output/jfeat_ljsryan/\" # added f0 feature, normalised over full range\n",
    "path = \"models/tronduo/\" # speaker vector\n",
    "iter = \"60000\" # number of iterations trained in the folder above\n",
    "outfolder = \"syn/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf8ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = path + \"checkpoint_\" + iter\n",
    "model = load_model(hparams)\n",
    "model.load_state_dict(torch.load(checkpoint_path)['state_dict'])\n",
    "_ = model.cuda().eval().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath, device):\n",
    "    assert os.path.isfile(filepath)\n",
    "    print(\"Loading '{}'\".format(filepath))\n",
    "    checkpoint_dict = torch.load(filepath, map_location=device)\n",
    "    print(\"Complete.\")\n",
    "    return checkpoint_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb35efaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hfg_path = 'models/hifigan/'\n",
    "checkpoint = 3100000\n",
    "config_file = hfg_path + 'config.json'\n",
    "checkpoint_file = hfg_path + 'g_' + str(checkpoint).zfill(8)\n",
    "with open(config_file) as f:\n",
    "    data = f.read()\n",
    "json_config = json.loads(data)\n",
    "h = AttrDict(json_config)\n",
    "torch.manual_seed(h.seed)\n",
    "generator = Generator(h).to(device)\n",
    "state_dict_g = load_checkpoint(checkpoint_file, device)\n",
    "generator.load_state_dict(state_dict_g['generator'])\n",
    "generator.eval()\n",
    "generator.remove_weight_norm()\n",
    "\n",
    "#from hifigandenoiser import Denoiser\n",
    "denoiser = Denoiser(generator, mode='zeros') # The other mode is normal/zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d44b18",
   "metadata": {},
   "source": [
    "### Text preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Meanwhile spring had passed well into summer.\",\n",
    "    \"They had come not to admire but to observe.\",\n",
    "    \"There were other farmhouses nearby.\",\n",
    "    \"Outside, only a handful of reporters remained.\",\n",
    "    \"Coverage of primary literature will follow.\",\n",
    "    \"The data are presented in lists and tables.\",\n",
    "    \"A third volume remains to be published.\",\n",
    "    \"At intervals an alumni directory is issued.\",\n",
    "    \"Let us differentiate a few of these ideas.\",\n",
    "    \"The system works as an impersonal mechanism.\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c4f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"si1905\", \"si1710\", \"si1664\", \"si1552\", \"si1445\",\n",
    "        \"si1442\", \"si1440\", \"si1297\", \"si1182\", \"si1083\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21588493",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts = [None]*len(texts)\n",
    "txt = [re.sub('[\\!.?]+','',tr) for tr in texts]\n",
    "txt = [re.sub(';','.',tr) for tr in txt]\n",
    "for i in range(len(texts)):\n",
    "    phon = g2p(txt[i])\n",
    "    for j, n in enumerate(phon):\n",
    "        if n == ' ':\n",
    "            phon[j] = '} {'\n",
    "    transcripts[i] = '{ '+' '.join(phon)+' }.'\n",
    "transcripts = [re.sub(r'(\\s+){ , }(\\s+)', ',', tr) for tr in transcripts]\n",
    "transcripts = [re.sub(r'(\\s+)?{ . }(\\s+)?', ';', tr) for tr in transcripts]\n",
    "#transcripts = [re.sub(r' ; ', ';', tr) for tr in transcripts]\n",
    "transcripts = [re.sub(r'{ ', '{', tr) for tr in transcripts]\n",
    "transcripts = [re.sub(r' }', '}', tr) for tr in transcripts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e223fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c2918f",
   "metadata": {},
   "source": [
    "### Load SGR tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "def init_torch():\n",
    "    torch.random.manual_seed(0)\n",
    "    device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.set_device(device)\n",
    "\n",
    "    print(torch.__version__)\n",
    "    print(torchaudio.__version__)\n",
    "    print(device)\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0daebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.fc2 = nn.Linear(512, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = init_torch()\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "w2v2 = bundle.get_model().to(device)\n",
    "w2v2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgr2 = MLPClassifier()\n",
    "sgr2.to(device)\n",
    "sgr2.load_state_dict(torch.load('models/sgr/sgr2_model_1gpu.pt'))\n",
    "sgr2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee2632",
   "metadata": {},
   "source": [
    "### Test a range of outcomes on a grid of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = [\"Filename\", \"f\", \"m\", \"f0_in\", \"dur\", \n",
    "                                  \"utt\", \"pr_f\", \"pr_m\", \"gap\"])\n",
    "outfolder = \"syn/\"\n",
    "out = True\n",
    "if out and not os.path.exists(outfolder):\n",
    "    os.makedirs(outfolder)\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8effa224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variation on speaker on a grid\n",
    "for i in tqdm(range(2)):\n",
    "    sequence = np.array(text_to_sequence(transcripts[i], ['english_cleaners']))[None, :]\n",
    "    sequence = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long()\n",
    "    for j in np.arange(0.,1.01,0.1):\n",
    "        for k in np.arange(0.,1.01,0.1):\n",
    "            for m in [-0.2, 0.2]:\n",
    "                speaks = torch.as_tensor([j,k]).unsqueeze(0).cuda()\n",
    "                pros = torch.as_tensor(m).unsqueeze(0).half().cuda()\n",
    "                #_, mel_outputs_postnet, _, _ = model.inference(sequence, speaks=speaks, pros=pros)\n",
    "                durat = 1000\n",
    "                while durat > 890:\n",
    "                    try:\n",
    "                        _, mel_outputs_postnet, _, _ = model.inference(sequence, speaks=speaks, pros=pros)\n",
    "                        durat = mel_outputs_postnet[0].size()[1]\n",
    "                    except:\n",
    "                        pass\n",
    "                melfl = mel_outputs_postnet.float()\n",
    "                y_g_hat = generator(melfl)\n",
    "                audio = denoiser(y_g_hat[0], strength=0.01).squeeze().half()\n",
    "                audio_out = audio.cpu().detach().numpy()\n",
    "                # generate output\n",
    "                filename = f'{filenames[i]}_F{int(round(100*j,0)):04}M{int(round(100*k,0)):04}f0_{int(round(100*m,0)):03}'\n",
    "                waveform = torchaudio.functional.resample(audio.unsqueeze(0).float(), 22050, bundle.sample_rate)\n",
    "                feats, _ = w2v2.extract_features(waveform.clone().to(device))\n",
    "                vec = torch.mean(feats[2], dim=1)\n",
    "                outputs = sgr2(vec).detach().cpu().numpy()\n",
    "                pf = 1 / (1+math.exp(-outputs[0][0]))\n",
    "                pm = 1 / (1+math.exp(-outputs[0][1]))\n",
    "\n",
    "                results = results.append({\"Filename\":filename, \"f\":100*j,\"m\":100*k,\"f0_in\":m,\n",
    "                                          \"dur\":np.round(len(audio_out)/hparams.sampling_rate,3), \n",
    "                                          \"utt\":files[i],\n",
    "                                         \"pr_f\":pf/(pf+pm), \"pr_m\":pm/(pf+pm), \"gap\":abs(pf-pm)/(pf+pm)},\n",
    "                                        ignore_index = True)\n",
    "                if out:\n",
    "                    print(filename)\n",
    "                    sf.write(outfolder + filename + '.wav',\n",
    "                                        audio_out.astype('float32'), hparams.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29650e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('test_results.csv', sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6acf4fc",
   "metadata": {},
   "source": [
    "## Create grid to display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b591c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Audio, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c42710",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfolder = \"syn/\"\n",
    "size = 9 # grid dimension\n",
    "mid = 50 # grid midpoint for features (0 corresponds to 1st percentile in training, 100 to 99th percentile)\n",
    "step = 10 # grid step size\n",
    "utt = 1\n",
    "f0 = 0.2\n",
    "high = \"100%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad839aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>.myclass { border:1px solid white ; font-size:70% ; grid-row-gap:0px}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a34bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_sound(b):\n",
    "    f = b.tooltip\n",
    "    audios.clear_output(wait=True)\n",
    "    with audios:\n",
    "        display(Audio(filename=outfolder + f\"{f}.wav\", autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for i in np.arange(round(mid-(size-1)/2*step),round(mid+(size-1)/2*step)+0.01,step):\n",
    "    for j in np.arange(round(i-(size-1)/2*step),round(i+(size-1)/2*step)+0.01,step):\n",
    "        files.append(f'{filenames[utt]}_F{int(round(i)):04}M{int(round(j)):04}f0_{int(round(f0*100,0)):03}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "buttons = [widgets.Button(description=\"\", tooltip=files[i], layout=widgets.Layout(width=\"100%\", height=high), button_style='primary').add_class('myclass') for i in range(size*size)]\n",
    "for i in range(size//2,size*size,size):\n",
    "    buttons[i].button_style = 'info'\n",
    "for i, button in enumerate(buttons):\n",
    "    button.on_click(play_sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b25dbe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_out = widgets.Output(layout={\"display\": \"flex\", \"flex_flow\": \"row wrap\", \"align_items\": \"flex-start\", \"margin\": \"0\"})\n",
    "for i in range(size):\n",
    "    grid = widgets.GridBox([widgets.Button(description=f\"{str(mid+(size//2-i)*step)}\", layout=widgets.Layout(width=\"100%\", height=high)).add_class('myclass')]\\\n",
    "                           +[widgets.Button(description=\"\", layout=widgets.Layout(width=\"100%\", height=high)).add_class('myclass')]*(size-1-i)\\\n",
    "                           +buttons[(size-1-i)*size:(size-i)*size]\\\n",
    "                           +[widgets.Button(description=\"\", layout=widgets.Layout(width=\"100%\", height=high)).add_class('myclass')]*i\\\n",
    "                           , layout=widgets.Layout(grid_template_columns=f\"repeat({2*size},1fr)\"), grid_gap='0px 0px')\n",
    "    grid_out.append_display_data(grid)\n",
    "# add x-axis\n",
    "xax = [widgets.Button(description=f\"{str(x)}\", layout=widgets.Layout(width=\"100%\", height=high)).add_class('myclass') for x in range(mid-(size-1)*step,mid+size*step,step)]\n",
    "grid = widgets.GridBox([widgets.Button(description='F/M', layout=widgets.Layout(width=\"100%\", height=high)).add_class('myclass')]\\\n",
    "                           +[widgets.Button(description=\"\", layout=widgets.Layout(width=\"100%\", height=high))]*(size-1-i)\\\n",
    "                           +xax, layout=widgets.Layout(grid_template_columns=f\"repeat({2*size},1fr)\"), grid_gap='0px 0px')\n",
    "grid_out.append_display_data(grid)\n",
    "display(grid_out)\n",
    "audios = widgets.Output(layout={'border': '1px solid black'})\n",
    "display(audios)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
